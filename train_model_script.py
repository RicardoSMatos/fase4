#!/usr/bin/env python3
"""
Script de treinamento do modelo GPT-2 para gera√ß√£o de hist√≥rias
Alternativa ao notebook train_model.ipynb
"""

import sys
from pathlib import Path

print("="*80)
print("TREINAMENTO GPT-2 PARA GERA√á√ÉO DE HIST√ìRIAS")
print("Tech Challenge Fase 4 - FIAP")
print("="*80 + "\n")

# Verifica lzma
print("üîç Verificando ambiente...")
try:
    import lzma
    print("‚úÖ M√≥dulo lzma dispon√≠vel")
except ImportError:
    print("‚ùå ERRO: M√≥dulo lzma n√£o encontrado!")
    print("\nSolu√ß√µes:")
    print("1. Use Python do Homebrew:")
    print("   brew install python@3.11")
    print("   /opt/homebrew/bin/python3.11 train_model_script.py")
    print("\n2. Use Google Colab (mais f√°cil)")
    sys.exit(1)

# Verifica e instala pacotes
print("\nüì¶ Verificando depend√™ncias...")
pacotes_necessarios = {
    'transformers': 'transformers',
    'datasets': 'datasets',
    'accelerate': 'accelerate',
    'torch': 'torch',
    'pandas': 'pandas',
    'matplotlib': 'matplotlib',
    'seaborn': 'seaborn',
    'tqdm': 'tqdm',
    'sklearn': 'scikit-learn'
}

pacotes_faltando = []
for modulo, pacote_pip in pacotes_necessarios.items():
    try:
        __import__(modulo)
    except ImportError:
        pacotes_faltando.append(pacote_pip)

if pacotes_faltando:
    print(f"‚ùå Faltam pacotes: {', '.join(pacotes_faltando)}")
    print(f"\nInstale com: pip install {' '.join(pacotes_faltando)}")
    sys.exit(1)

print("‚úÖ Todas as depend√™ncias instaladas\n")

# Imports
import torch
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    Trainer,
    TrainingArguments,
    DataCollatorForLanguageModeling
)
from datasets import Dataset

sys.path.append(str(Path.cwd()))
from data.sample_stories import STORIES_DATASET, format_story_for_training

# Configura√ß√µes
MODEL_NAME = "pierreguillou/gpt2-small-portuguese"
OUTPUT_DIR = "./models/fine_tuned_gpt2_story_generator"
device = "cuda" if torch.cuda.is_available() else "cpu"

print(f"üñ•Ô∏è Dispositivo: {device}")
if device == "cpu":
    print("‚ö†Ô∏è  Treino sem GPU vai demorar ~1-3 horas")
    print("üí° Dica: Use Google Colab para GPU gratuita\n")

# Carrega modelo
print(f"üì• Carregando modelo: {MODEL_NAME}")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)

if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
    model.config.pad_token_id = model.config.eos_token_id

model.to(device)
print(f"‚úÖ Modelo carregado ({model.num_parameters():,} par√¢metros)\n")

# Prepara dataset
print(f"üìö Preparando dataset ({len(STORIES_DATASET)} hist√≥rias)...")
formatted_stories = [{'text': format_story_for_training(s)} for s in STORIES_DATASET]
dataset = Dataset.from_list(formatted_stories)
dataset = dataset.train_test_split(test_size=0.2, seed=42)

def tokenize_function(examples):
    return tokenizer(
        examples['text'],
        truncation=True,
        max_length=1024,
        padding='max_length',
        return_tensors='pt'
    )

tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=['text'])
print(f"‚úÖ Dataset pronto (treino: {len(dataset['train'])}, valida√ß√£o: {len(dataset['test'])})\n")

# Configura√ß√£o de treino
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    num_train_epochs=5,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    gradient_accumulation_steps=4,
    learning_rate=5e-5,
    warmup_steps=500,
    logging_dir='./logs',
    logging_steps=50,
    eval_strategy="steps",
    eval_steps=200,
    save_strategy="steps",
    save_steps=200,
    save_total_limit=3,
    fp16=True if device == "cuda" else False,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    report_to="none",
    seed=42
)

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset['train'],
    eval_dataset=tokenized_dataset['test'],
    data_collator=data_collator,
)

# Treina
print("üöÄ Iniciando treinamento...")
print("‚è∞ Tempo estimado: 5-15 min (GPU) ou 1-3h (CPU)\n")

train_result = trainer.train()

print("\n" + "="*80)
print("‚úÖ TREINO CONCLU√çDO!")
print("="*80)
print(f"Loss final: {train_result.training_loss:.4f}\n")

# Avalia
print("üìä Avaliando modelo...")
eval_results = trainer.evaluate()
for key, value in eval_results.items():
    print(f"  ‚Ä¢ {key}: {value:.4f}")

# Salva
print(f"\nüíæ Salvando modelo em: {OUTPUT_DIR}")
model.save_pretrained(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)
print("‚úÖ Modelo salvo com sucesso!")

# Teste r√°pido
print("\nüß™ Teste r√°pido de gera√ß√£o:")
prompt = """G√™nero: Fic√ß√£o Cient√≠fica
Personagens: Dr. Silva
Cen√°rio: Esta√ß√£o espacial
Tom: Aventuroso

Hist√≥ria: """

inputs = tokenizer(prompt, return_tensors='pt').to(device)
with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_length=len(inputs['input_ids'][0]) + 200,
        temperature=0.9,
        do_sample=True
    )
historia = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(historia[:300] + "...\n")

print("="*80)
print("üéâ PROCESSO COMPLETO!")
print(f"Modelo salvo em: {OUTPUT_DIR}")
print("="*80)
