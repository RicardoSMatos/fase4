"""
Classe para carregar e gerenciar o modelo de gera√ß√£o de texto.
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from typing import Optional
import os


class TextGenerator:
    """
    Classe para gerenciar o modelo de gera√ß√£o de texto.
    """
    
    def __init__(
        self, 
        model_name: Optional[str] = None,
        device: Optional[str] = None
    ):
        """
        Inicializa o gerador de texto.
        
        Args:
            model_name: Nome ou caminho do modelo (None = auto-detecta)
            device: Dispositivo para executar o modelo (cuda/cpu)
        """
        # Auto-detecta melhor modelo dispon√≠vel
        if model_name is None:
            model_name = self._detect_best_model()
        
        self.model_name = model_name
        
        # Determina o dispositivo
        if device is None:
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = device
            
        print(f"üöÄ Carregando modelo: {model_name}")
        print(f"üíª Dispositivo: {self.device}")
        
        # Carrega tokenizer e modelo
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(model_name)
        self.model.to(self.device)
        self.model.eval()
        
        # Configura pad token se n√£o existir
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            
        print("‚úÖ Modelo carregado com sucesso!")
    
    @staticmethod
    def _detect_best_model() -> str:
        """
        Detecta o melhor modelo dispon√≠vel.
        
        Prioridade:
        1. Modelo fine-tuned local (se existir)
        2. BLOOM 560M (melhor para gera√ß√£o criativa multil√≠ngue)
        3. GPT-2 Portuguese Small (fallback r√°pido)
        
        Returns:
            Nome ou caminho do modelo
        """
        # Verifica se existe modelo fine-tuned local
        local_model = "./models/fine_tuned_gpt2_story_generator"
        if os.path.exists(local_model) and os.path.isdir(local_model):
            config_file = os.path.join(local_model, "config.json")
            if os.path.exists(config_file):
                print(f"üéØ Modelo fine-tuned encontrado: {local_model}")
                return local_model
        
        # Usa BLOOM por padr√£o (melhor que GPT-2 Small)
        print("‚ö†Ô∏è Modelo fine-tuned n√£o encontrado. Usando modelo pr√©-treinado...")
        
        # BLOOM 560M - melhor balan√ßo qualidade/velocidade
        default_model = "bigscience/bloom-560m"
        
        print(f"üì• Usando modelo: {default_model}")
        print("üí° BLOOM √© um modelo multil√≠ngue de alta qualidade treinado pela BigScience")
        print("   Suporta portugu√™s e √© melhor para gera√ß√£o criativa que GPT-2 Small")
        return default_model
    
    @staticmethod
    def get_available_models():
        """
        Retorna lista de modelos dispon√≠veis para sele√ß√£o.
        
        Returns:
            Dict com modelos dispon√≠veis e validados
        """
        models = {}
        
        # Verifica modelo fine-tuned
        local_model = "./models/fine_tuned_gpt2_story_generator"
        if os.path.exists(local_model) and os.path.isdir(local_model):
            config_file = os.path.join(local_model, "config.json")
            if os.path.exists(config_file):
                models["üéØ Fine-tuned GPT-2 (Treinado em Hist√≥rias) - MELHOR"] = local_model
        
        # Modelos VALIDADOS do HuggingFace para gera√ß√£o criativa
        
        # GPT-2 Portuguese Small (baseline)
        models["üìù GPT-2 Portuguese Small (124M) - R√°pido"] = "pierreguillou/gpt2-small-portuguese"
        
        # BLOOM - Modelo grande multil√≠ngue com portugu√™s (RECOMENDADO)
        models["üå∏ BLOOM 560M (Multil√≠ngue) - Melhor Qualidade"] = "bigscience/bloom-560m"
        
        # BLOOM maior
        models["ÔøΩ BLOOM 1B1 (Multil√≠ngue) - Alta Qualidade (Lento)"] = "bigscience/bloom-1b1"
        
        # Alternativa: mGPT (multil√≠ngue incluindo portugu√™s)
        models["üåç mGPT 1.3B (Multil√≠ngue) - Boa Qualidade"] = "ai-forever/mGPT"
        
        return models
    
    def generate(
        self,
        prompt: str,
        max_length: int = 200,
        temperature: float = 0.9,
        top_k: int = 50,
        top_p: float = 0.95,
        repetition_penalty: float = 1.3,
        num_return_sequences: int = 1,
        do_sample: bool = True,
        no_repeat_ngram_size: int = 3
    ) -> list:
        """
        Gera texto a partir de um prompt.
        
        Args:
            prompt: Texto inicial
            max_length: Comprimento m√°ximo do texto gerado
            temperature: Controla a aleatoriedade (maior = mais criativo) [0.8-1.1 ideal]
            top_k: N√∫mero de tokens candidatos [40-60 ideal]
            top_p: Probabilidade acumulada (nucleus sampling) [0.90-0.95 ideal]
            repetition_penalty: Penalidade para repeti√ß√µes [1.2-1.5 ideal]
            num_return_sequences: N√∫mero de sequ√™ncias a gerar
            do_sample: Se True, usa amostragem; se False, usa greedy decoding
            no_repeat_ngram_size: Evita repeti√ß√£o de n-gramas (3 = trigrams)
            
        Returns:
            Lista de textos gerados
        """
        # Tokeniza o prompt
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512
        ).to(self.device)
        
        # Gera o texto
        with torch.no_grad():
            outputs = self.model.generate(
                inputs.input_ids,
                max_length=max_length,
                temperature=temperature,
                top_k=top_k,
                top_p=top_p,
                repetition_penalty=repetition_penalty,
                num_return_sequences=num_return_sequences,
                do_sample=do_sample,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                attention_mask=inputs.attention_mask,
                no_repeat_ngram_size=no_repeat_ngram_size
            )
        
        # Decodifica os resultados
        generated_texts = []
        for output in outputs:
            text = self.tokenizer.decode(output, skip_special_tokens=True)
            generated_texts.append(text)
        
        return generated_texts
    
    def save_model(self, output_dir: str):
        """
        Salva o modelo e tokenizer.
        
        Args:
            output_dir: Diret√≥rio de sa√≠da
        """
        os.makedirs(output_dir, exist_ok=True)
        self.model.save_pretrained(output_dir)
        self.tokenizer.save_pretrained(output_dir)
        print(f"‚úÖ Modelo salvo em: {output_dir}")
    
    @classmethod
    def from_pretrained(cls, model_path: str, device: Optional[str] = None):
        """
        Carrega um modelo salvo localmente.
        
        Args:
            model_path: Caminho do modelo
            device: Dispositivo para executar o modelo
            
        Returns:
            Inst√¢ncia de TextGenerator
        """
        return cls(model_name=model_path, device=device)
