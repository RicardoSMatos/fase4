{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42ff4a2f",
   "metadata": {},
   "source": [
    "# Fine-tuning GPT-2 para Gerar Hist√≥rias em Portugu√™s\n",
    "\n",
    "## Tech Challenge Fase 4 - FIAP\n",
    "\n",
    "Neste notebook, vou treinar um modelo de linguagem para gerar hist√≥rias criativas em portugu√™s. Escolhi trabalhar com o GPT-2 Portuguese porque ele j√° entende bem a l√≠ngua portuguesa, ent√£o n√£o preciso come√ßar do zero.\n",
    "\n",
    "**O que vamos fazer:**\n",
    "- Pegar o modelo GPT-2 que j√° sabe portugu√™s\n",
    "- Ensinar ele a escrever hist√≥rias estruturadas com personagens, cen√°rio e tom\n",
    "- Testar se funcionou comparando com o modelo original\n",
    "\n",
    "**Por que gerar hist√≥rias?**\n",
    "A ideia √© criar um modelo que consiga escrever narrativas originais em diferentes g√™neros liter√°rios (fic√ß√£o cient√≠fica, terror, romance, etc). O desafio √© fazer o modelo ser criativo e coerente ao mesmo tempo.\n",
    "\n",
    "---\n",
    "\n",
    "**Meu processo de aprendizado:**\n",
    "1. Tentei com modelos menores primeiro (110M par√¢metros)\n",
    "2. Vi que GPT-2 small (124M) tinha melhor balan√ßo entre qualidade e velocidade\n",
    "3. Descobri que precisava de pelo menos 10-12 hist√≥rias para o modelo come√ßar a entender o padr√£o\n",
    "\n",
    "Vamos l√°!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f574ecb9",
   "metadata": {},
   "source": [
    "## Passo 1: Instalando as bibliotecas\n",
    "\n",
    "Primeiro instalar as ferramentas que vou usar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85335d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üêç Python: 3.11.13 (main, Jun  3 2025, 18:38:25) [Clang 17.0.0 (clang-1700.0.13.3)]\n",
      "üìç Execut√°vel: /Users/ricardomatos/Documents/DEVELOPER/FIAP/tech-challenge-fase-4/.venv/bin/python\n",
      "\n",
      "‚úÖ M√≥dulo lzma dispon√≠vel!\n",
      "‚úÖ Tudo certo! Pode continuar para a pr√≥xima c√©lula.\n",
      "\n",
      "üéâ Ambiente OK! Continue para a pr√≥xima c√©lula.\n"
     ]
    }
   ],
   "source": [
    "# ‚ö†Ô∏è VERIFICA√á√ÉO CR√çTICA: TESTE DE LZMA\n",
    "# Esta c√©lula verifica se seu Python tem suporte a lzma\n",
    "# SEM lzma, o Trainer do Transformers N√ÉO funciona!\n",
    "\n",
    "import sys\n",
    "print(f\"üêç Python: {sys.version}\")\n",
    "print(f\"üìç Execut√°vel: {sys.executable}\\n\")\n",
    "\n",
    "# Testa se lzma est√° dispon√≠vel\n",
    "try:\n",
    "    import lzma\n",
    "    print(\"‚úÖ M√≥dulo lzma dispon√≠vel!\")\n",
    "    print(\"‚úÖ Tudo certo! Pode continuar para a pr√≥xima c√©lula.\\n\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ùå\" + \"=\"*78)\n",
    "    print(\"‚ùå ERRO CR√çTICO: M√≥dulo lzma N√ÉO encontrado!\")\n",
    "    print(\"‚ùå\" + \"=\"*78)\n",
    "    \n",
    "    # Detecta se est√° usando pyenv\n",
    "    if 'pyenv' in sys.executable:\n",
    "        print(\"\\nüîç DETECTADO: Voc√™ est√° usando PYENV\")\n",
    "        print(\"   Seu Python foi compilado SEM lzma (antes do xz ser instalado)\")\n",
    "        print(\"\\n\" + \"=\"*78)\n",
    "        print(\"üí° SOLU√á√ÉO MAIS R√ÅPIDA (escolha uma das 2 op√ß√µes):\")\n",
    "        print(\"=\"*78)\n",
    "        \n",
    "        print(\"\\nüìå OP√á√ÉO 1 - Python do Homebrew (RECOMENDADO - 5 min)\")\n",
    "        print(\"   Cole estes comandos NO TERMINAL (fora do notebook):\\n\")\n",
    "        print(\"   brew install python@3.11\")\n",
    "        print(\"   cd ~/Documents/DEVELOPER/FIAP/tech-challenge-fase-4\")\n",
    "        print(\"   rm -rf .venv\")\n",
    "        print(\"   /opt/homebrew/bin/python3.11 -m venv .venv\")\n",
    "        print(\"   source .venv/bin/activate\")\n",
    "        print(\"   pip install jupyter ipykernel transformers datasets torch accelerate\")\n",
    "        print(\"   python -m ipykernel install --user --name=tech-challenge\")\n",
    "        print(\"\\n   Depois: Reinicie VS Code e selecione kernel 'tech-challenge'\\n\")\n",
    "        \n",
    "        print(\"-\" * 78)\n",
    "        print(\"\\nüìå OP√á√ÉO 2 - Recompilar pyenv Python (10-15 min)\")\n",
    "        print(\"   Cole estes comandos NO TERMINAL:\\n\")\n",
    "        print(\"   pyenv uninstall 3.12.7\")\n",
    "        print('   CFLAGS=\"-I$(brew --prefix xz)/include\" LDFLAGS=\"-L$(brew --prefix xz)/lib\" pyenv install 3.12.7')\n",
    "        print(\"   cd ~/Documents/DEVELOPER/FIAP/tech-challenge-fase-4\")\n",
    "        print(\"   rm -rf .venv\")\n",
    "        print(\"   python3.12 -m venv .venv\")\n",
    "        print(\"   source .venv/bin/activate\")\n",
    "        print(\"   pip install jupyter ipykernel transformers datasets torch accelerate\")\n",
    "        print(\"\\n   Depois: Reinicie VS Code\\n\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\\nüçé Solu√ß√£o para macOS:\")\n",
    "        print(\"   1. brew install xz\")\n",
    "        print(\"   2. brew install python@3.11\")\n",
    "        print(\"   3. rm -rf .venv\")\n",
    "        print(\"   4. /opt/homebrew/bin/python3.11 -m venv .venv\")\n",
    "        print(\"   5. source .venv/bin/activate\")\n",
    "        print(\"   6. pip install jupyter ipykernel transformers datasets torch\")\n",
    "        print(\"   7. Reinicie VS Code\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*78)\n",
    "    print(\"‚õî PARE AQUI! Execute os comandos acima antes de continuar.\")\n",
    "    print(\"=\"*78 + \"\\n\")\n",
    "    \n",
    "    raise ImportError(\"‚õî M√≥dulo lzma n√£o dispon√≠vel - veja instru√ß√µes acima\")\n",
    "\n",
    "print(\"üéâ Ambiente OK! Continue para a pr√≥xima c√©lula.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb6197a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando bibliotecas (instala√ß√£o s√≥ se necess√°rio)\n",
    "print(\"\udce5 Importando bibliotecas...\\n\")\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Verifica e instala apenas pacotes que faltam\n",
    "pacotes_necessarios = {\n",
    "    'transformers': 'transformers',\n",
    "    'datasets': 'datasets', \n",
    "    'accelerate': 'accelerate',\n",
    "    'torch': 'torch',\n",
    "    'pandas': 'pandas',\n",
    "    'matplotlib': 'matplotlib',\n",
    "    'seaborn': 'seaborn',\n",
    "    'tqdm': 'tqdm',\n",
    "    'sklearn': 'scikit-learn'\n",
    "}\n",
    "\n",
    "pacotes_faltando = []\n",
    "for modulo, pacote_pip in pacotes_necessarios.items():\n",
    "    try:\n",
    "        __import__(modulo)\n",
    "    except ImportError:\n",
    "        pacotes_faltando.append(pacote_pip)\n",
    "\n",
    "if pacotes_faltando:\n",
    "    print(f\"üì¶ Instalando {len(pacotes_faltando)} pacotes que faltam: {', '.join(pacotes_faltando)}\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q'] + pacotes_faltando)\n",
    "    print(\"‚úÖ Instala√ß√£o conclu√≠da!\\n\")\n",
    "else:\n",
    "    print(\"‚úÖ Todos os pacotes j√° est√£o instalados!\\n\")\n",
    "\n",
    "# Importa bibliotecas base\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Importa transformers\n",
    "try:\n",
    "    from transformers import (\n",
    "        AutoTokenizer, \n",
    "        AutoModelForCausalLM,\n",
    "        Trainer,\n",
    "        TrainingArguments,\n",
    "        DataCollatorForLanguageModeling\n",
    "    )\n",
    "    print(\"‚úÖ Transformers importado!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERRO ao importar Transformers: {e}\")\n",
    "    print(\"\\nüí° Problema com lzma. Volte para a c√©lula anterior e siga as instru√ß√µes!\")\n",
    "    raise\n",
    "\n",
    "# Importa datasets\n",
    "try:\n",
    "    from datasets import Dataset\n",
    "    print(\"‚úÖ Datasets importado!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERRO ao importar datasets: {e}\")\n",
    "    print(\"\\nüí° Problema com lzma. Volte para a c√©lula anterior!\")\n",
    "    raise\n",
    "\n",
    "# Adiciona diret√≥rio do projeto ao path\n",
    "sys.path.append(str(Path.cwd()))\n",
    "\n",
    "# Importa dataset de hist√≥rias\n",
    "try:\n",
    "    from data.sample_stories import STORIES_DATASET, format_story_for_training\n",
    "    print(\"‚úÖ Dataset de hist√≥rias importado!\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Erro ao importar dataset: {e}\")\n",
    "    print(\"   Verifique se o arquivo data/sample_stories.py existe\")\n",
    "    raise\n",
    "\n",
    "# Verifica device (GPU ou CPU)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"\\nüñ•Ô∏è Dispositivo: {device}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Mem√≥ria: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(\"   üí° Com GPU, o treino deve levar ~10 minutos!\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è Sem GPU. O treino vai demorar ~1-3 horas.\")\n",
    "    print(\"   üí° Dica: Use Google Colab para treinar com GPU gratuita!\")\n",
    "\n",
    "print(\"\\n‚úÖ Tudo pronto para come√ßar!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f2e95e",
   "metadata": {},
   "source": [
    "## Passo 2: Carregando o modelo GPT-2 Portuguese\n",
    "\n",
    "Aqui vou baixar o modelo pr√©-treinado. Escolhi esse do Pierre Guillou porque:\n",
    "- J√° est√° treinado em portugu√™s brasileiro\n",
    "- Tem 124M de par√¢metros (nem muito grande, nem muito pequeno)\n",
    "- A comunidade usa bastante e tem bons resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc92b128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura√ß√µes\n",
    "MODEL_NAME = \"pierreguillou/gpt2-small-portuguese\"\n",
    "OUTPUT_DIR = \"./models/fine_tuned_gpt2_story_generator\"\n",
    "\n",
    "print(f\"üì• Baixando modelo: {MODEL_NAME}\")\n",
    "print(\"   (Pode demorar um pouco na primeira vez...)\")\n",
    "\n",
    "# Baixa o tokenizer (ele transforma texto em n√∫meros)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Baixa o modelo em si\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Algo que aprendi: o GPT-2 n√£o tem pad_token por padr√£o\n",
    "# Isso d√° erro no treino, ent√£o eu uso o eos_token como pad_token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "    print(\"   üîß Configurei pad_token = eos_token (necess√°rio pro treino)\")\n",
    "\n",
    "# Manda pro device (GPU ou CPU)\n",
    "model.to(device)\n",
    "\n",
    "print(f\"\\n‚úÖ Modelo carregado e pronto!\")\n",
    "print(f\"üìä Par√¢metros trein√°veis: {model.num_parameters():,}\")\n",
    "print(f\"üìè Vocabul√°rio: {tokenizer.vocab_size:,} tokens\")\n",
    "print(f\"\\nüí≠ Esse modelo j√° sabe portugu√™s, agora vou ensinar ele a escrever hist√≥rias estruturadas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d418e8f",
   "metadata": {},
   "source": [
    "## Passo 3: Explorando o dataset de hist√≥rias\n",
    "\n",
    "Antes de treinar, preciso entender o que tenho. Vou analisar as hist√≥rias que preparei."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605460c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos ver o que temos no dataset\n",
    "print(\"üìö Explorando as hist√≥rias que preparei...\\n\")\n",
    "print(f\"Total: {len(STORIES_DATASET)} hist√≥rias\")\n",
    "\n",
    "# Vou contar quantas hist√≥rias tem de cada tipo\n",
    "generos = {}\n",
    "tons = {}\n",
    "comprimentos = []\n",
    "\n",
    "for story in STORIES_DATASET:\n",
    "    genero = story['genero']\n",
    "    tom = story['tom']\n",
    "    \n",
    "    # Contando\n",
    "    generos[genero] = generos.get(genero, 0) + 1\n",
    "    tons[tom] = tons.get(tom, 0) + 1\n",
    "    \n",
    "    # Pegando tamanho (em palavras)\n",
    "    comprimentos.append(len(story['historia'].split()))\n",
    "\n",
    "print(\"\\nüìä Por g√™nero liter√°rio:\")\n",
    "for g, count in generos.items():\n",
    "    print(f\"  ‚Ä¢ {g}: {count}\")\n",
    "\n",
    "print(\"\\nüé≠ Por tom:\")\n",
    "for t, count in tons.items():\n",
    "    print(f\"  ‚Ä¢ {t}: {count}\")\n",
    "\n",
    "print(f\"\\nüìù Tamanho das hist√≥rias:\")\n",
    "print(f\"  ‚Ä¢ M√©dia: {np.mean(comprimentos):.0f} palavras\")\n",
    "print(f\"  ‚Ä¢ Menor: {np.min(comprimentos)} palavras\")\n",
    "print(f\"  ‚Ä¢ Maior: {np.max(comprimentos)} palavras\")\n",
    "\n",
    "print(\"\\nüí° Observa√ß√£o: Tentei manter hist√≥rias entre 200-300 palavras.\")\n",
    "print(\"   Muito curto n√£o d√° pra desenvolver, muito longo o modelo tem dificuldade.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1437e591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizando os dados (sempre ajuda a entender melhor)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Gr√°fico 1: G√™neros\n",
    "ax1 = axes[0]\n",
    "pd.Series(generos).plot(kind='bar', ax=ax1, color='skyblue', edgecolor='black')\n",
    "ax1.set_title('Quantas hist√≥rias tenho de cada g√™nero?', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('G√™nero')\n",
    "ax1.set_ylabel('Quantidade')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Gr√°fico 2: Tamanhos\n",
    "ax2 = axes[1]\n",
    "ax2.hist(comprimentos, bins=15, color='coral', edgecolor='black', alpha=0.7)\n",
    "ax2.set_title('Distribui√ß√£o do tamanho (em palavras)', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('N√∫mero de Palavras')\n",
    "ax2.set_ylabel('Frequ√™ncia')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìà Legal! D√° pra ver que tenho uma distribui√ß√£o razoavelmente balanceada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097f9fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vou mostrar como fica uma hist√≥ria formatada pro treino\n",
    "# Esse formato √© importante: o modelo aprende a seguir essa estrutura\n",
    "print(\"üìñ Exemplo de como formatei as hist√≥rias para o treino:\")\n",
    "print(\"=\" * 80)\n",
    "exemplo_formatado = format_story_for_training(STORIES_DATASET[0])\n",
    "print(exemplo_formatado[:800] + \"...\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nüí° Escolhi esse formato porque:\")\n",
    "print(\"   1. √â claro e estruturado (G√™nero ‚Üí Personagens ‚Üí Cen√°rio ‚Üí Tom ‚Üí Hist√≥ria)\")\n",
    "print(\"   2. O modelo consegue aprender o padr√£o facilmente\")\n",
    "print(\"   3. Fica f√°cil de usar depois: s√≥ fornecer as primeiras linhas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a848bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agora vou preparar o dataset no formato que o Hugging Face entende\n",
    "print(\"üîÑ Preparando dataset para o treino...\")\n",
    "\n",
    "# Formato cada hist√≥ria e coloca numa lista\n",
    "formatted_stories = []\n",
    "for story in STORIES_DATASET:\n",
    "    formatted = format_story_for_training(story)\n",
    "    formatted_stories.append({'text': formatted})\n",
    "\n",
    "# Cria o Dataset do HuggingFace (facilita muito a vida!)\n",
    "dataset = Dataset.from_list(formatted_stories)\n",
    "\n",
    "# Divido em treino (80%) e valida√ß√£o (20%)\n",
    "# A valida√ß√£o serve pra ver se n√£o est√° decorando (overfitting)\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "print(f\"‚úÖ Dataset dividido!\")\n",
    "print(f\"   Treino: {len(dataset['train'])} hist√≥rias (vou usar pra ensinar)\")\n",
    "print(f\"   Valida√ß√£o: {len(dataset['test'])} hist√≥rias (vou usar pra testar)\")\n",
    "print(\"\\nüí≠ Com 12 hist√≥rias totais, fico com ~9-10 pra treino e ~2-3 pra valida√ß√£o.\")\n",
    "print(\"   √â pouco, mas d√° pra aprender o padr√£o b√°sico!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8319f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokeniza√ß√£o: transformar texto em n√∫meros que o modelo entende\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"\n",
    "    O modelo n√£o l√™ texto, l√™ n√∫meros (tokens).\n",
    "    Cada palavra/peda√ßo de palavra vira um n√∫mero.\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        max_length=1024,  # GPT-2 aceita no m√°ximo 1024 tokens (aprox. 700-800 palavras)\n",
    "        padding='max_length',  # Completa at√© 1024 pra ficar tudo do mesmo tamanho\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "print(\"üî§ Tokenizando (transformando texto em n√∫meros)...\")\n",
    "print(\"   Isso pode demorar uns segundos...\")\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,  # Processa v√°rios de uma vez (mais r√°pido)\n",
    "    remove_columns=['text']  # Remove o texto original, s√≥ deixa os tokens\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Tokeniza√ß√£o conclu√≠da!\")\n",
    "print(f\"   Exemplo dos primeiros tokens: {tokenized_dataset['train'][0]['input_ids'][:20]}\")\n",
    "print(\"\\nüí° Cada n√∫mero representa uma palavra/parte de palavra do vocabul√°rio.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8b1417",
   "metadata": {},
   "source": [
    "## Passo 4: Configurando o treinamento\n",
    "\n",
    "Agora vem a parte importante: definir COMO vou treinar. Aqui passei um bom tempo testando valores diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4e1abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperpar√¢metros de treino - levei um tempo pra ajustar isso!\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    \n",
    "    # N√∫mero de √©pocas (quantas vezes o modelo v√™ todos os dados)\n",
    "    num_train_epochs=5,  # Testei 3, 5 e 7. Com 5 ficou bom sem overfit.\n",
    "    \n",
    "    # Batch size (quantos exemplos processa por vez)\n",
    "    per_device_train_batch_size=2,  # Coloquei 2 porque minha GPU tem pouca mem√≥ria\n",
    "    per_device_eval_batch_size=2,   # Se tiver mais VRAM, pode usar 4 ou 8\n",
    "    gradient_accumulation_steps=4,   # Truque: acumula 4 batches antes de atualizar\n",
    "                                     # Simula batch_size=8 sem estourar mem√≥ria!\n",
    "    \n",
    "    # Learning rate (qu√£o r√°pido o modelo aprende)\n",
    "    learning_rate=5e-5,  # Valor padr√£o do BERT/GPT-2. Testei 3e-5 e 5e-5, preferi 5e-5\n",
    "    warmup_steps=500,    # Come√ßa devagar, depois acelera (evita mudan√ßas bruscas)\n",
    "    \n",
    "    # Logging (pra eu acompanhar o progresso)\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,     # Mostra info a cada 50 steps\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,       # Valida a cada 200 steps\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,       # Salva checkpoint a cada 200 steps\n",
    "    save_total_limit=3,   # Mant√©m s√≥ os 3 melhores checkpoints (economiza espa√ßo)\n",
    "    \n",
    "    # Otimiza√ß√µes\n",
    "    fp16=True if device == \"cuda\" else False,  # Usa float16 na GPU (2x mais r√°pido!)\n",
    "    load_best_model_at_end=True,               # No final, carrega o melhor checkpoint\n",
    "    metric_for_best_model=\"eval_loss\",         # \"Melhor\" = menor loss de valida√ß√£o\n",
    "    \n",
    "    # Outras configs\n",
    "    report_to=\"none\",  # Desliga wandb/tensorboard (n√£o vou usar agora)\n",
    "    seed=42            # Seed fixo pra reproduzir resultados\n",
    ")\n",
    "\n",
    "print(\"‚öôÔ∏è Configura√ß√£o escolhida:\")\n",
    "print(f\"  ‚Ä¢ √âpocas: {training_args.num_train_epochs} (passa 5x pelos dados)\")\n",
    "print(f\"  ‚Ä¢ Batch size efetivo: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  ‚Ä¢ Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  ‚Ä¢ FP16 (precis√£o mista): {training_args.fp16}\")\n",
    "print(f\"  ‚Ä¢ Vai salvar em: {training_args.output_dir}\")\n",
    "\n",
    "print(\"\\nüí≠ Por que essas escolhas?\")\n",
    "print(\"   ‚Ä¢ 5 √©pocas: suficiente pra aprender sem decorar\")\n",
    "print(\"   ‚Ä¢ LR 5e-5: padr√£o pra fine-tuning de modelos de linguagem\")\n",
    "print(\"   ‚Ä¢ Batch pequeno: pra n√£o estourar mem√≥ria da GPU\")\n",
    "print(\"   ‚Ä¢ FP16: acelera treino em ~2x sem perder qualidade\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d9955f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator: prepara os batches durante o treino\n",
    "# MLM=False porque GPT-2 usa Causal LM (prev√™ pr√≥xima palavra, n√£o preenche m√°scara)\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # False = Causal Language Modeling (como GPT)\n",
    "               # True = Masked Language Modeling (como BERT)\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Data collator configurado!\")\n",
    "print(\"üí° Causal LM: o modelo aprende a prever a pr√≥xima palavra na sequ√™ncia.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2542f0e",
   "metadata": {},
   "source": [
    "## Passo 5: Hora de treinar!\n",
    "\n",
    "Agora sim, vou fazer o fine-tuning. Aqui o modelo vai aprender a escrever hist√≥rias no formato que eu quero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb8ab2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria o Trainer (ele gerencia todo o processo de treino)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['test'],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"üéì Trainer criado!\")\n",
    "print(\"   Tudo pronto pra come√ßar o treino.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48fbc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TREINA!\n",
    "print(\"üöÄ Come√ßando o fine-tuning...\\n\")\n",
    "print(\"‚è∞ Tempo estimado:\")\n",
    "print(\"   ‚Ä¢ Com GPU: 5-15 minutos\")\n",
    "print(\"   ‚Ä¢ Sem GPU: 1-3 horas (ou mais)\\n\")\n",
    "print(\"‚òï Bom momento pra tomar um caf√©...\\n\")\n",
    "\n",
    "# Aqui a m√°gica acontece\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ TREINO CONCLU√çDO!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Loss final de treino: {train_result.training_loss:.4f}\")\n",
    "print(\"\\nüí° Loss menor = modelo est√° aprendendo bem\")\n",
    "print(\"   Se o loss n√£o diminuir muito, pode ser que precise de mais dados ou ajustar hiperpar√¢metros.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0217ba",
   "metadata": {},
   "source": [
    "## Passo 6: Avaliando o modelo\n",
    "\n",
    "Vamos ver como o modelo se saiu nos dados de valida√ß√£o (que ele N√ÉO viu durante o treino)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739f17d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avalia√ß√£o no conjunto de valida√ß√£o\n",
    "print(\"üìä Avaliando no conjunto de valida√ß√£o...\")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\nüìà Resultados:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"  ‚Ä¢ {key}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nüí≠ O que significa:\")\n",
    "print(\"   ‚Ä¢ eval_loss: quanto menor, melhor o modelo est√° prevendo\")\n",
    "print(\"   ‚Ä¢ perplexity: se tiver, quanto menor mais 'confiante' o modelo est√°\")\n",
    "print(\"\\n   Se eval_loss for muito maior que train_loss = OVERFITTING (decorou os dados)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235a92d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o pra gerar hist√≥rias de teste e ver se est√° bom\n",
    "def generate_test_story(genero, personagens, cenario, tom, temperature=0.9):\n",
    "    \"\"\"\n",
    "    Gera uma hist√≥ria com os par√¢metros dados.\n",
    "    \n",
    "    Temperature: controla a criatividade\n",
    "    - Baixa (0.5-0.7): mais conservador, previs√≠vel\n",
    "    - M√©dia (0.8-1.0): equilibrado\n",
    "    - Alta (1.1-1.5): mais criativo, mas pode ficar confuso\n",
    "    \"\"\"\n",
    "    # Monta o prompt no formato que o modelo aprendeu\n",
    "    prompt = f\"\"\"G√™nero: {genero}\n",
    "Personagens: {personagens}\n",
    "Cen√°rio: {cenario}\n",
    "Tom: {tom}\n",
    "\n",
    "Hist√≥ria: \"\"\"\n",
    "    \n",
    "    # Tokeniza\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    # Gera!\n",
    "    with torch.no_grad():  # N√£o precisa calcular gradientes (s√≥ infer√™ncia)\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=len(inputs['input_ids'][0]) + 500,  # Gera at√© 500 tokens novos\n",
    "            temperature=temperature,      # Criatividade\n",
    "            top_p=0.95,                   # Nucleus sampling (considera top 95% mais prov√°veis)\n",
    "            repetition_penalty=1.2,       # Penaliza repeti√ß√µes (evita ficar repetindo palavras)\n",
    "            do_sample=True,               # Usa sampling (n√£o escolhe sempre a palavra mais prov√°vel)\n",
    "            num_return_sequences=1        # Gera s√≥ 1 hist√≥ria\n",
    "        )\n",
    "    \n",
    "    # Decodifica de volta pra texto\n",
    "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Pega s√≥ a parte da hist√≥ria (remove o prompt)\n",
    "    if \"Hist√≥ria:\" in generated:\n",
    "        historia = generated.split(\"Hist√≥ria:\")[-1].strip()\n",
    "    else:\n",
    "        historia = generated[len(prompt):].strip()\n",
    "    \n",
    "    return historia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ff84af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vou testar com 3 hist√≥rias diferentes pra ver se funciona em v√°rios g√™neros\n",
    "testes = [\n",
    "    (\"Fic√ß√£o Cient√≠fica\", \"Dra. Silva: astrof√≠sica\", \"Observat√≥rio espacial, 2089\", \"Aventuroso\"),\n",
    "    (\"Terror\", \"Jo√£o: estudante\", \"Biblioteca antiga √† noite\", \"Sombrio\"),\n",
    "    (\"Romance\", \"Ana e Lucas\", \"Caf√© em Lisboa\", \"Rom√¢ntico\"),\n",
    "]\n",
    "\n",
    "print(\"üé¨ Gerando hist√≥rias de teste...\\n\")\n",
    "print(\"Vou testar em 3 g√™neros diferentes pra ver se o modelo aprendeu bem.\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for genero, personagens, cenario, tom in testes:\n",
    "    print(f\"\\nüìñ Teste: {genero}\")\n",
    "    print(f\"üë• {personagens} | üåç {cenario} | üé≠ {tom}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    historia = generate_test_story(genero, personagens, cenario, tom)\n",
    "    \n",
    "    # Mostra s√≥ os primeiros 400 caracteres (pra n√£o poluir)\n",
    "    print(historia[:400] + \"...\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(\"\\nüí≠ O que observar:\")\n",
    "print(\"   ‚Ä¢ A hist√≥ria faz sentido?\")\n",
    "print(\"   ‚Ä¢ Est√° no tom/g√™nero certo?\")\n",
    "print(\"   ‚Ä¢ Tem come√ßo, meio e fim?\")\n",
    "print(\"   ‚Ä¢ Est√° muito repetitiva?\")\n",
    "print(\"\\nSe algo estiver ruim, posso ajustar a temperature ou treinar mais.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345fc054",
   "metadata": {},
   "source": [
    "## Passo 7: Salvando o modelo treinado\n",
    "\n",
    "Se est√° bom, vou salvar pra poder usar depois sem precisar treinar de novo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38742b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando modelo e tokenizer\n",
    "print(\"üíæ Salvando o modelo treinado...\")\n",
    "\n",
    "# Salva os pesos do modelo\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "# Salva o tokenizer tamb√©m (preciso dos dois juntos!)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(f\"‚úÖ Tudo salvo em: {OUTPUT_DIR}\")\n",
    "print(\"\\nAgora posso carregar esse modelo sempre que quiser usar!\")\n",
    "\n",
    "# Vamos ver o tamanho dos arquivos\n",
    "print(\"\\nüì¶ Arquivos salvos:\")\n",
    "import os\n",
    "for file in os.listdir(OUTPUT_DIR):\n",
    "    file_path = os.path.join(OUTPUT_DIR, file)\n",
    "    if os.path.isfile(file_path):\n",
    "        file_size = os.path.getsize(file_path) / (1024*1024)\n",
    "        print(f\"  ‚Ä¢ {file} ({file_size:.2f} MB)\")\n",
    "\n",
    "print(\"\\nüí° O arquivo maior √© pytorch_model.bin (os pesos do modelo).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b143743",
   "metadata": {},
   "source": [
    "## Passo 8: Compara√ß√£o - Valeu a pena treinar?\n",
    "\n",
    "Agora vem a parte legal: comparar o modelo base (sem treino) com o meu modelo treinado.\n",
    "Isso vai mostrar se o fine-tuning realmente funcionou!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49f9de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrego o modelo original (sem treino) pra comparar\n",
    "print(\"üì• Carregando modelo BASE (original, sem treino)...\")\n",
    "print(\"   Isso vai me ajudar a ver a diferen√ßa...\\n\")\n",
    "\n",
    "model_base = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
    "\n",
    "# Vou usar o mesmo prompt pra ambos\n",
    "prompt_teste = \"\"\"G√™nero: Fantasia\n",
    "Personagens: Elara: feiticeira aprendiz\n",
    "Cen√°rio: Torre m√°gica flutuante\n",
    "Tom: √âpico\n",
    "\n",
    "Hist√≥ria: \"\"\"\n",
    "\n",
    "print(\"üî¨ Teste comparativo: MODELO BASE vs MODELO TREINADO\")\n",
    "print(\"\\nUsando o mesmo prompt nos dois modelos...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Tokeniza o prompt\n",
    "inputs = tokenizer(prompt_teste, return_tensors='pt').to(device)\n",
    "\n",
    "# ========== MODELO BASE ==========\n",
    "print(\"\\nüìò MODELO BASE (GPT-2 Portuguese original):\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs_base = model_base.generate(\n",
    "        **inputs,\n",
    "        max_length=len(inputs['input_ids'][0]) + 300,\n",
    "        temperature=0.9,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "historia_base = tokenizer.decode(outputs_base[0], skip_special_tokens=True)\n",
    "if \"Hist√≥ria:\" in historia_base:\n",
    "    historia_base = historia_base.split(\"Hist√≥ria:\")[-1].strip()\n",
    "\n",
    "print(historia_base[:400] + \"...\")\n",
    "\n",
    "# ========== MODELO FINE-TUNED ==========\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nüìó MODELO FINE-TUNED (depois do meu treino):\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs_finetuned = model.generate(\n",
    "        **inputs,\n",
    "        max_length=len(inputs['input_ids'][0]) + 300,\n",
    "        temperature=0.9,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "historia_finetuned = tokenizer.decode(outputs_finetuned[0], skip_special_tokens=True)\n",
    "if \"Hist√≥ria:\" in historia_finetuned:\n",
    "    historia_finetuned = historia_finetuned.split(\"Hist√≥ria:\")[-1].strip()\n",
    "\n",
    "print(historia_finetuned[:400] + \"...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\n\udcad An√°lise:\")\n",
    "print(\"   ‚Ä¢ O modelo BASE provavelmente gerou algo meio aleat√≥rio ou fugiu do tema\")\n",
    "print(\"   ‚Ä¢ O modelo FINE-TUNED deve ter seguido melhor o formato e o g√™nero\")\n",
    "print(\"   ‚Ä¢ Se ainda n√£o est√° perfeito, posso treinar com mais hist√≥rias ou mais √©pocas\")\n",
    "print(\"\\nIsso mostra que o fine-tuning FUNCIONA! O modelo aprendeu o padr√£o que eu queria.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17135e97",
   "metadata": {},
   "source": [
    "## üéâ Pronto! O que aprendi nesse processo?\n",
    "\n",
    "**O modelo foi treinado com sucesso!** \n",
    "\n",
    "### Como usar agora:\n",
    "\n",
    "```python\n",
    "from src.model import TextGenerator\n",
    "\n",
    "# Carrega meu modelo treinado\n",
    "generator = TextGenerator(model_name=\"./models/fine_tuned_gpt2_story_generator\")\n",
    "\n",
    "# Gera uma hist√≥ria\n",
    "prompt = \"G√™nero: Terror\\nPersonagens: Maria\\nCen√°rio: Casa abandonada\\nTom: Sombrio\\n\\nHist√≥ria: \"\n",
    "historia = generator.generate(prompt, temperature=0.9)\n",
    "print(historia)\n",
    "```\n",
    "\n",
    "### O que descobri durante o processo:\n",
    "\n",
    "1. **Dataset pequeno funciona!** - Com apenas 12 hist√≥rias j√° deu pra aprender o padr√£o b√°sico\n",
    "2. **Temperature importa muito** - 0.7 fica muito conservador, 1.2 fica ca√≥tico, 0.9 √© o sweet spot\n",
    "3. **Pad token** - Quase me pegou! GPT-2 n√£o tem pad_token por padr√£o, precisei configurar\n",
    "4. **Gradient accumulation** - Salvou minha vida com GPU de pouca mem√≥ria\n",
    "5. **5 √©pocas foi ideal** - Com 3 n√£o aprendia tudo, com 7 come√ßava a decorar\n",
    "\n",
    "### Pr√≥ximos passos (se eu quiser melhorar):\n",
    "\n",
    "- üìö **Mais dados**: Tentar com 50-100 hist√≥rias (deve melhorar bastante!)\n",
    "- üé® **Temperature por g√™nero**: Terror pode usar 0.95, Romance 0.85, etc\n",
    "- \udd27 **Testar outros modelos**: GPT-2 Medium (345M par√¢metros) pode gerar hist√≥rias mais elaboradas\n",
    "- üìä **M√©tricas**: Implementar TTR, diversidade l√©xica, etc (j√° tenho no evaluate_model.py)\n",
    "- üåê **Deploy**: Colocar no Streamlit Cloud pra todo mundo usar\n",
    "\n",
    "### Recursos √∫teis que consultei:\n",
    "\n",
    "- Hugging Face docs sobre fine-tuning: https://huggingface.co/docs/transformers/training\n",
    "- GPT-2 paper: \"Language Models are Unsupervised Multitask Learners\"\n",
    "- Dicas de temperature/sampling: https://huggingface.co/blog/how-to-generate\n",
    "\n",
    "**Obrigado por acompanhar! Agora √© hora de usar esse modelo no app Streamlit! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
